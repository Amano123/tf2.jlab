{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import io\n",
    "import glob\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.readers.TextLineDatasetV2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.data.TextLineDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "九州地方 の ローカルヒーロ 一覧 きゅうしゅう ちほう の ローカルヒーロ いち らん は 九州地方 で 作ら れ た ローカルヒーロ の 一覧 記事\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"./sudachi_dataset/バイク.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "    print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path     :./sudachi_dataset/\n",
      "file name:['バイク.txt', '公開企業.txt']\n",
      "file id  :['0', '1']\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = \"./sudachi_dataset/\"\n",
    "files_path = \"./sudachi_dataset/\"\n",
    "DATA_FILE_NAMES = ['バイク.txt', '公開企業.txt']\n",
    "\n",
    "DATA_FILE_ID = ['0', '1']\n",
    "print(f\"path     :{FILE_PATH}\")\n",
    "print(f\"file name:{DATA_FILE_NAMES}\")\n",
    "print(f\"file id  :{DATA_FILE_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> コモドール 社 の コンピューター は 当時 フィンランド で 最も 人気 の ある コンピューター  製品 で あっ た <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(w,num=None):\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    if num == None:\n",
    "        return w\n",
    "    else:\n",
    "        return w,num\n",
    "        \n",
    "sample_text  = 'コモドール 社 の コンピューター は 当時 フィンランド で 最も 人気 の ある コンピューター  製品 で あっ た'\n",
    "print(preprocess_sentence(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeler(example, index):\n",
    "    return example, tf.cast(index, tf.int64)\n",
    "labeled_data_sets = []\n",
    "for i, file_name in enumerate(DATA_FILE_NAMES, 0):\n",
    "    lines_dataset = tf.data.TextLineDataset(FILE_PATH + file_name).map(preprocess_sentence)\n",
    "    labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "    labeled_data_sets.append(labeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<MapDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
       " <MapDataset shapes: ((), ()), types: (tf.string, tf.int64)>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labeled_data = labeled_data_sets[0]\n",
    "for labeled_dataset in labeled_data_sets[1:]:\n",
    "    all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
    "\n",
    "all_labeled_data = all_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text,label in all_labeled_data:\n",
    "    print(text.numpy().decode(\"utf-8\"))\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "vocabulary_0 = set()\n",
    "vocabulary_1 = set()\n",
    "\n",
    "for text_tensor, label in all_labeled_data:\n",
    "    some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "    vocabulary_set.update(some_tokens)\n",
    "    if label.numpy() == 0:\n",
    "        vocabulary_0.update(some_tokens)\n",
    "    else:\n",
    "        vocabulary_1.update(some_tokens)\n",
    "        \n",
    "vocab_size   = len(vocabulary_set)\n",
    "vocab_size_0 = len(vocabulary_0)\n",
    "vocab_size_1 = len(vocabulary_1)\n",
    "\n",
    "print(f\"full:{vocab_size}\")\n",
    "print(f\"0:{vocab_size_0}\")\n",
    "print(f\"1:{vocab_size_1}\")\n",
    "print(f\"0 + 1 = {len(vocabulary_0 | vocabulary_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_mapper(token,label):\n",
    "    token = encoder.encode(token.numpy())\n",
    "    label= np.array([label])\n",
    "    return token,label\n",
    "\n",
    "@tf.function\n",
    "def tf_encode(token,label):\n",
    "    return tf.py_function(dataset_mapper, [token,label], [tf.int64, tf.int64])\n",
    "\n",
    "new_dataset = all_labeled_data.map(tf_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, label in all_labeled_data:\n",
    "    print(token.numpy().decode(\"utf-8\"))\n",
    "    print(dataset_mapper(token,label))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "new_dataset = new_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1],[-1]),drop_remainder=True)\n",
    "new_dataset = new_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.Input(shape=(None,), batch_size=BATCH_SIZE)\n",
    "embedded = tf.keras.layers.Embedding(encoder.vocab_size+1, 128)(X)\n",
    "lstm, forward_h, forward_c, backward_h, backward_c = tf.keras.layers.Bidirectional(\n",
    "      tf.keras.layers.LSTM(128,return_sequences=True,return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "  )(embedded)\n",
    "state_h = tf.keras.layers.Concatenate()([forward_h, backward_h]) # 重みを結合\n",
    "context,attention_weights = Attention(128)(lstm,state_h) # ここにAttentionレイヤを挟む\n",
    "fully_connected = tf.keras.layers.Dense(units=128, activation='relu')(context)\n",
    "Y = tf.keras.layers.Dense(1, activation='sigmoid',name='final_layer')(fully_connected)\n",
    "\n",
    "model = tf.keras.Model(inputs=X, outputs=Y)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(new_dataset,epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./test_model/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.keras.Model(inputs=model.input, outputs=[model.output, model.get_layer('attention').output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_0 = sum([encoder.encode(vocab) for vocab in vocabulary_0],[])\n",
    "vocab_1 = sum([encoder.encode(vocab) for vocab in vocabulary_1],[])\n",
    "print(vocab_0)\n",
    "print(vocab_1)\n",
    "\n",
    "score_0 = test.predict([vocab_0])\n",
    "weght_0 = score_0[1][1]\n",
    "\n",
    "score_1 = test.predict([vocab_1])\n",
    "weght_1 = score_1[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_text_0 = []\n",
    "for i in vocab_0:\n",
    "    for x,t in enumerate(vocabulary_set,1):\n",
    "        # print(x,t)\n",
    "        if x == int(i):\n",
    "            token_to_text_0.append(t)\n",
    "token_to_text_1 = []\n",
    "for i in vocab_1:\n",
    "    for x,t in enumerate(vocabulary_set,1):\n",
    "        # print(x,t)\n",
    "        if x == int(i):\n",
    "            token_to_text_1.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.DataFrame([token_to_text_0, np.ravel(weght_0[0])], index=['text', 'weight']).T\n",
    "df_1 = pd.DataFrame([token_to_text_1, np.ravel(weght_1[0])], index=['text', 'weight']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0.to_csv(\"./メディアミックス.csv\")\n",
    "df_1.to_csv(\"./公開企業.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
